{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Introduction\n\nIn this notebook, we demonstrate the steps needed create a model for predicting Remaining Useful Life for turbofan engines based on data collected by devices and routed into storage via the IoT Hub. The notebook assumes that you have complete the device data generation steps from the [IoT  Edge for Machine Learning](aka.ms/IoTEdgeMLPaper). The data generated from the devices needs to be in an Azure Storage account blob container in the same Azure Subscription as you will use to create the Azure Machine Learning service workspace using this notebook.  \n\nThe steps we will complete in this notebooks are:\n   1. Create a Machine Learning service workspace for managing the experiments, compute, and models for this sample\n   1. Load training data from Azure Storage\n   1. Prepare the data for training the model\n   1. Explore the data \n   1. Remotely train the model\n   1. Test the model using test data\n\nThe intent is not to provide an extensive coverage of machine learning in Azure as that is covered in much depth elsewhere [here for example](https://github.com/Azure/MachineLearningNotebooks), but to demonstrate how machine learning can be used with IoT Edge."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Setup notebook\n\nPlease ensure that you are running the notebook under the Python 3.6 kernal. Intall fastavro and setup interactive shell to display output nicely."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install fastavro\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n%matplotlib inline",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting fastavro\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/db/c0f0e7d5176c35985ec25b652e78a4072c34b353070b2acadccd8637c712/fastavro-0.21.15-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n\u001b[K    100% |████████████████████████████████| 1.2MB 11.6MB/s ta 0:00:01\n\u001b[?25hInstalling collected packages: fastavro\nSuccessfully installed fastavro-0.21.15\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Set global properties\n\nThese properties will be used throughout the notebook.\n   * `AZURE_SUBSCRIPTION_ID` - the Azure subscription containing the storage account where device data has been uploaded. We will create the Machine Learning service workspace (ml workspace) in this subscription.\n   * `ML_WORKSPACE_NAME`  name to give the ml workspace\n   * `AZURE_IOT_HUB_NAME` - name of the Azure IoT Hub used in creating the device data using the DeviceHarness.  See [IoT  Edge for Machine Learning](aka.ms/IoTEdgeMLPaper) for details.\n   * `STORAGE_ACCOUNT_NAME` - name of the Azure Storage account where device data was routed via IoT Hub.\n   * `STORAGE_ACCOUNT_KEY` - access key for the Azure Storage account\n   * `STORAGE_ACCOUNT_CONTAINER` - name of Azure Storage blob container where device data was routed via IoT Hub."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "AZURE_SUBSCRIPTION_ID = ''\nML_WORKSPACE_NAME = ''\nAZURE_IOT_HUB_NAME = ''\nSTORAGE_ACCOUNT_NAME= ''\nSTORAGE_ACCOUNT_KEY= ''\nSTORAGE_ACCOUNT_CONTAINER= ''",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create a workspace\n\n## What is an Azure ML Workspace and Why Do I Need One?\n\nAn Azure ML workspace is an Azure resource that organizes and coordinates the actions of many other Azure resources to assist in executing and sharing machine learning workflows. In particular, an Azure ML workspace coordinates storage, databases, and compute resources providing added functionality for machine learning experimentation, operationalization, and the monitoring of operationalized models.\n\nIn addition to creating the workspace, the cell below writes a file, config.json, to a ./aml_config/config.json, which allows the Workspace object to be reloaded later.\n\n\n\n\n><font color=gray>_Note: currently Workspaces are supported in the following regions: eastus2, eastus,westcentralus, southeastasia, westeurope, australiaeast, westus2, southcentralus_</font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\nworkspace_name = ML_WORKSPACE_NAME\nsubscription_id = AZURE_SUBSCRIPTION_ID\nresource_group = workspace_name\n\nws = Workspace.create(name=workspace_name,\n                      subscription_id=subscription_id,    \n                      resource_group=resource_group,\n                      create_resource_group=True,\n                      location='westus2' # or other supported Azure region\n                     )\n\nws.write_config(path='./aml_config')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Workspace details\n\nIf there is no workspace object in the notebook, the  `Workspace.from_config()` reads the file **aml_config/config.json** and loads the detail. It then prints the Workspace details."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\nimport pandas as pd\nfrom azureml.core import Workspace\n\nif 'ws' not in globals():\n    ws = Workspace.from_config()\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data=output, index=['']).T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Download data from storage\n\nThe first step toward creating a model for RUL is to explore the data and understand its shape. We will download the data for this purpose, realizing that in the case of larger data sets only a sample of the data would be used at this step."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Register storage account\n\nThe Datastore is a convenient construct associated the Workspace to upload/download data, and interact with it from remote compute targets. Register the Azure Storage account and container where device data was routed by IoT Hub using the information about the storage container provided at the beginning of the notebook.\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Datastore\n\nds = Datastore.register_azure_blob_container(workspace=ws,\n                                             datastore_name='turbofan',\n                                             container_name=STORAGE_ACCOUNT_CONTAINER,\n                                             account_name=STORAGE_ACCOUNT_NAME,\n                                             account_key=STORAGE_ACCOUNT_KEY,\n                                             create_if_not_exists=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use Datastore to download data\n\nUse the Datastore to download the files to the local machine. The prefix is the top level path to download, which should be the name of the IoT Hub. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds.download(target_path=\"./data/download\", prefix=AZURE_IOT_HUB_NAME)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Load train data\n\nThe data we just downloaded represent a series of messages sent by each device stored in [Apache Avro](https://avro.apache.org/docs/current/)(avro) format. We will use the fastavro package to deserialize the records from the avro files.\nHere is an example deserialized record from the avro file. \n\n```json \n{\n    \"EnqueuedTimeUtc\": \"2018-12-01T01: 16: 22.0950000Z\",\n    \"Properties\": {},\n    \"SystemProperties\": {\n        \"connectionDeviceId\": \"Client_3\",\n        \"connectionAuthMethod\": {\n            \"scope\": \"device\",\n            \"type\": \"sas\",\n            \"issuer\": \"iothub\",\n            \"acceptingIpFilterRule\": null\n        },\n        \"connectionDeviceGenerationId\": \"636791290544434625\",\n        \"contentType\": \"application/json\",\n        \"contentEncoding\": \"utf-8\",\n        \"enqueuedTime\": \"2018-12-01T01: 16: 22.0950000Z\"\n    },\n    \"Body\": b'{\n        \"CycleTime\": 1,\n        \"OperationalSetting1\": -0.0001,\n        \"OperationalSetting2\": 0.0001,\n        \"OperationalSetting3\": 100.0,\n        \"Sensor1\": 518.67,\n        \"Sensor2\": 642.03,\n       //Sensor 3-19 ommitted for brevity\n        \"Sensor20\": 38.99,\n        \"Sensor21\": 23.296\n    }\n}```\n\nTaken together the messages represent a time series of data for multiple engines. Each engine is operating normally at the start of each time series, and develops a fault at some point during the series. The fault grows in magnitude until system failure (i.e. the failure point for the engine is the final cycle in the set). The remaining useful life (RUL) is therefore expressed as: \n\n$$RUL_{current} = Cycle_{max} - Cycle_{current}$$\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create utils for loading data from avro files\n\nDefine a set of utility methods for loading the data from the avro files. We use thes utilities to load the locally downloaded data. Later in the notebook, these same utilities will form the basis of data processing for remote training (see **Train regression using Azure AutoMl and remote execution** below)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile ./utils.py\n\nimport glob\nimport json\nimport pandas as pd\n\nfrom fastavro import reader\nfrom os.path import isfile\n\n# parse connectionDeviceId and return the int part\n# (e.g. Client_1 becomes 1)\ndef get_unit_num (unit_record):\n    unit = unit_record[\"connectionDeviceId\"]\n    return int(unit.split('_')[1])\n\n# create data row from avro file record\ndef load_cycle_row(record):\n    json_body = record[\"Body\"].decode()\n    row = json.loads(json_body)\n    row.update({'Unit': get_unit_num(record[\"SystemProperties\"])})\n    row.update({'QueueTime': pd.to_datetime(record[\"EnqueuedTimeUtc\"])})\n    return row\n\n# add row to data frame\ndef append_df(base_df, append_df):\n    if(base_df is None):\n        base_df = pd.DataFrame(append_df)\n    else:\n        base_df = base_df.append(append_df, ignore_index=True)\n    return base_df\n\n# sort rows and columns in dataframe\ndef sort_and_index(index_data):\n    #sort rows and reset index\n    index_data.sort_values(by=['Unit', 'CycleTime'], inplace=True)\n    index_data.reset_index(drop=True, inplace=True)\n    \n    #fix up column sorting for convenience in notebook\n    sorted_cols = ([\"Unit\",\"CycleTime\", \"QueueTime\"] \n                   + [\"OperationalSetting\"+str(i) for i in range(1,4)] \n                   + [\"Sensor\"+str(i) for i in range(1,22)])\n\n    return index_data[sorted_cols]\n\n# load data from an avro file and return a dataframe\ndef load_avro_file(avro_file_name):\n    with open(avro_file_name, 'rb') as fo:\n        file_df = None\n        avro_reader = reader(fo)\n        print (\"load records from file: %s\" % avro_file_name)\n        for record in avro_reader:\n            row = load_cycle_row(record)\n            file_df = append_df(base_df=file_df, append_df=[row])\n        return file_df\n\n# load data from all avro files in given dir \ndef load_avro_directory(avro_dir_name):\n    dir_df = None\n    for file_name in glob.iglob(avro_dir_name, recursive=True):\n        if isfile(file_name):\n            file_df = load_avro_file(file_name)\n            dir_df = append_df(base_df=dir_df, append_df=file_df)\n    print(\"loaded %d records\" % dir_df.shape[0])\n    return sort_and_index(dir_df)\n\n# add max cycle to each row in the data\ndef add_maxcycle(data_frame):\n    # cleanup column if it already exists\n    if 'MaxCycle' in data_frame.columns:\n        data_frame.drop('MaxCycle', axis=1, inplace=True)\n\n    total_cycles = data_frame.groupby(['Unit']).agg({'CycleTime' : 'max'}).reset_index()\n    total_cycles.rename(columns = {'CycleTime' : 'MaxCycle'}, inplace = True)\n    return data_frame.merge(total_cycles, how = 'left', left_on = 'Unit', right_on = 'Unit')\n\n# return a remaining useful life class based on RUL\ndef classify_rul(rul):\n     if (rul <= 25):\n          return 'F25'\n     elif (rul <= 75):\n          return 'F75'\n     elif (rul <= 150):\n          return 'F150'\n     else:\n          return 'Full'\n    \n# add remaining useful life and remaing useful life class\n# to each row in the data\ndef add_rul(data_frame):\n    data_frame = add_maxcycle(data_frame)\n    \n    if 'RUL' in data_frame.columns:\n        data_frame.drop('RUL', axis=1, inplace=True)\n    data_frame['RUL'] = data_frame.apply(lambda r: int(r['MaxCycle'] - r['CycleTime']), axis = 1)\n\n    if 'RulClass' in data_frame.columns:\n        data_frame.drop('RulClass', axis=1, inplace=True)\n    data_frame['RulClass'] = data_frame.apply(lambda r: classify_rul(r['RUL']), axis = 1)\n    \n    return data_frame\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use utils to load data from download directory"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import utils\n\ntrain_pd =  utils.load_avro_directory('./data/download/**/*')\n\ntrain_pd.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Calculate remaining useful life and RUL class labels\n\nAdd RUL for regression training and RulClass for classification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_pd = utils.add_rul(train_pd)\n\ncols = ['Unit', 'CycleTime', 'MaxCycle', 'RUL', 'RulClass']\n#show first 5 rows\ntrain_pd[cols].head(5)\n\n#show last 5 rows for engine 3\ntrain_pd[train_pd['Unit'] == 3][cols].tail(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Display train data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_pd.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Explore the data\n\nVisualize the data to start to get a sense of how features like sensor measurements and operations settings relate to remaining useful life (RUL)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Sensor readings and RUL\n\nCreate a scatterplot for each sensor measurement vs. RUL. Notice that some measurements (e.g. sensor 2) seem to be correlated strongly to RUL whereas other measurements (e.g. sensor 1) stay constant throughout the life of the engine.\n\n    \n><font color=gray>_Note: the data is limited to the first 10 engine units for speed of rendering_</font>"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#select the data to plot\nplotData = train_pd.query('Unit < 10');\n\nsns.set()\ng = sns.PairGrid(data=plotData,\n                x_vars = ['RUL'],\n                y_vars = [\"Sensor\"+str(i) for i in range(1,22)],\n                hue=\"Unit\",\n                height=3,\n                aspect=2.5,\n                palette=\"Paired\")\ng = g.map(plt.scatter, alpha=0.3)\ng = g.set(xlim=(300,0))\ng = g.add_legend()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Operational settings and RUL\n\nCreate a scatterplot for each operation setting vs. RUL. Operational settings do not seem to correlate with RUL.\n    \n><font color=gray>_Note: the data is limited to the first 10 engine units for speed of rendering_</font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport utils\n\nplotData = train_pd.query('Unit < 10');\nsns.set()\ng = sns.PairGrid(data=plotData,\n                x_vars = ['RUL'],\n                y_vars = [\"OperationalSetting\"+str(i) for i in range(1,4)],\n                hue=\"Unit\",\n                size=3,\n                aspect=2.5,\n                palette=\"Paired\")\ng = g.map(plt.scatter, alpha=0.3)\ng = g.set(xlim=(300,0))\ng = g.add_legend()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Train model using Azure AutoMl and remote execution\n\nIn this section, we will use the Azure Machine Learning service to build a model to predict remaining useful life."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##  Create remote compute target\n\nAzure ML Managed Compute is a managed service that enables data scientists to train machine learning models on clusters of Azure virtual machines, including VMs with GPU support. This code creates an Azure Managed Compute cluster if it does not already exist in your workspace. \n\n **Creation of the cluster takes approximately 5 minutes.** If the cluster is already in the workspace this code uses it and skips the creation process."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\nCLUSTER_NAME = \"mlturbo\"\n\n# choose a name for your cluster\nbatchai_cluster_name = CLUSTER_NAME + \"gpu\"\ncluster_min_nodes = 1\ncluster_max_nodes = 3\nvm_size = \"STANDARD_NC6\" #NC6 is GPU-enabled\n      \ncts = ws.compute_targets\nif batchai_cluster_name in cts:\n    found = True\n    print('Found existing compute target...%s' % batchai_cluster_name)\n    compute_target = cts[batchai_cluster_name]\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n                                                               # vm_priority = 'lowpriority', #optional\n                                                                min_nodes = cluster_min_nodes, \n                                                                max_nodes = cluster_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, batchai_cluster_name, provisioning_config)\n    \n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n    # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n    compute_target.status.serialize()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Create a regression model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Configure run settings\n\nCreate a DataReferenceConfiguration object to inform the system what data folder to download to the compute target. The path_on_compute should be an absolute path to ensure that the data files are downloaded only once.  The get_data method should use the same path to access the data files."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Setup DataReference"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.automl import AutoMLConfig\nfrom azureml.core.runconfig import DataReferenceConfiguration\n\ndr = DataReferenceConfiguration(datastore_name=ds.name, \n                   path_on_datastore=AZURE_IOT_HUB_NAME, \n                   path_on_compute='/tmp/azureml_runs',\n                   mode='download', # download files from datastore to compute target\n                   overwrite=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Update run settings"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n# create a new RunConfig object\nconda_run_config = RunConfiguration(framework=\"python\")\n\n# Set compute target to the Azure ML managed compute\nconda_run_config.target = compute_target\n\n# set the data reference of the run coonfiguration\nconda_run_config.data_references = {ds.name: dr}\n\n#specify package dependencies needed to load data and train the model\ncd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]', 'fastavro'], conda_packages=['numpy'])\nconda_run_config.environment.python.conda_dependencies = cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create data retrieval script\n\nRemote execution requires a .py file containing a get_data() function that will be used to retrieve data from the mounted storage.  We will create the file for retrieving data by copying the utils.py file to our script folder as get_data.py.  Then we will append a get_data(), which uses the utility methods for data loading, into the newly created get_data.py.  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a directory "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = './turbofan-regression'\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create get data script"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create the script by copyting utils.py to the script_folder\nimport shutil\nshutil.copyfile('utils.py', script_folder + '/get_data.py')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# append the get_data method to the newly created get_data.py\n%%writefile -a $script_folder/get_data.py\n\ndef get_data():\n    #for the sake of simplicity use all sensors as training features for the model\n    features = [\"Sensor\"+str(i) for i in range(1,22)]\n    train_pd = load_avro_directory('/tmp/azureml_runs/**/*')\n    train_pd = add_rul(train_pd)\n    y_train = train_pd['RUL'].values\n    X_train = train_pd[features].values\n    return { \"X\" : X_train, \"y\" : y_train}\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Run the experiment on Azure ML compute \n### Instantiate AutoML\n\nIn the interest of time, the cell below uses a short iteration timeout, **1 min**, and a small number of iterations, **10**. Longer iteration timeouts and a greater number of iterations will yield better results"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import logging\nfrom azureml.train.automl import AutoMLConfig\n\n#name project folder and experiment\nexperiment_name = 'turbofan-regression-remote'\n\nautoml_settings = {\n    \"iteration_timeout_minutes\": 1,\n    \"iterations\": 10,\n    \"n_cross_validations\": 10,\n    \"primary_metric\": 'spearman_correlation',\n    \"max_cores_per_iteration\": -1,\n    \"enable_ensembling\": True,\n    \"ensemble_iterations\": 5,\n    \"verbosity\": logging.INFO,\n    \"preprocess\": True,\n    \"enable_tf\": True,\n    \"auto_blacklist\": True\n}\n\nAutoml_config = AutoMLConfig(task = 'regression',\n                             debug_log = 'auto-regress.log',\n                             path=script_folder,\n                             run_configuration=conda_run_config,\n                             data_script = script_folder + \"/get_data.py\",\n                             **automl_settings\n                            )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Run the experiment\n\nRun the experiment on the remote compute target and show results as the runs execute. Assuming you have kept the auto_ml settings set in the notebook this step will take several minutes.  If you have increased the number of iterations of the iteration timeout it will take longer."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.experiment import Experiment\nfrom azureml.widgets import RunDetails\n\nexperiment=Experiment(ws, experiment_name)\nregression_run = experiment.submit(Automl_config, show_output=False)\nRunDetails(regression_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Explore the results\n\nExplore the results of the automatic training using the run details."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Reconstitute a run\nGiven the long running nature of running the experiment the notebook may have been closed or timed out.  In that case, to retrieve the run from the run id set the value of `run_id` to the run_id of the experiment. We use `AutoMLRun` from `azureml.train.automl.run`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.automl.run import AutoMLRun\nfrom azureml.core.experiment import Experiment\nfrom azureml.core import Workspace\n\nrun_id = 'AutoML_xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n\nif 'regression_run' not in globals():\n    ws = Workspace.from_config()\n    experiment_name = 'turbofan-regression-remote'\n    experiment=Experiment(ws, experiment_name)\n    regression_run = AutoMLRun(experiment = experiment, \n                               run_id = run_id)\n\nregression_run.id",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "'AutoML_e1d6a076-6b98-4222-a0c1-acdeb861c9e8'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Retrieve all iterations\n\nView the experiment history and see individual metrics for each iteration run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "children = list(regression_run.get_children())\nmetricslist = {}\nfor run in children:\n    properties = run.get_properties()\n    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n    metricslist[int(properties['iteration'])] = metrics\n\nimport pandas as pd\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>explained_variance</th>\n      <td>0.598114</td>\n      <td>0.390203</td>\n      <td>0.337219</td>\n      <td>0.538539</td>\n      <td>0.556496</td>\n      <td>0.590431</td>\n      <td>0.569029</td>\n      <td>0.587016</td>\n      <td>0.313204</td>\n      <td>0.597787</td>\n    </tr>\n    <tr>\n      <th>mean_absolute_error</th>\n      <td>41.158197</td>\n      <td>55.180982</td>\n      <td>57.217501</td>\n      <td>45.047540</td>\n      <td>44.289862</td>\n      <td>42.028387</td>\n      <td>43.595751</td>\n      <td>42.269064</td>\n      <td>58.244588</td>\n      <td>41.221683</td>\n    </tr>\n    <tr>\n      <th>median_absolute_error</th>\n      <td>29.480241</td>\n      <td>47.278671</td>\n      <td>48.135341</td>\n      <td>33.973801</td>\n      <td>33.622797</td>\n      <td>31.027954</td>\n      <td>32.487259</td>\n      <td>31.280734</td>\n      <td>49.154011</td>\n      <td>29.660043</td>\n    </tr>\n    <tr>\n      <th>normalized_mean_absolute_error</th>\n      <td>0.075938</td>\n      <td>0.101810</td>\n      <td>0.105567</td>\n      <td>0.083114</td>\n      <td>0.081716</td>\n      <td>0.077543</td>\n      <td>0.080435</td>\n      <td>0.077987</td>\n      <td>0.107462</td>\n      <td>0.076055</td>\n    </tr>\n    <tr>\n      <th>normalized_median_absolute_error</th>\n      <td>0.054392</td>\n      <td>0.087230</td>\n      <td>0.088811</td>\n      <td>0.062682</td>\n      <td>0.062035</td>\n      <td>0.057247</td>\n      <td>0.059940</td>\n      <td>0.057714</td>\n      <td>0.090690</td>\n      <td>0.054723</td>\n    </tr>\n    <tr>\n      <th>normalized_root_mean_squared_error</th>\n      <td>0.105218</td>\n      <td>0.129355</td>\n      <td>0.134858</td>\n      <td>0.112526</td>\n      <td>0.110313</td>\n      <td>0.106007</td>\n      <td>0.108745</td>\n      <td>0.106449</td>\n      <td>0.137277</td>\n      <td>0.105187</td>\n    </tr>\n    <tr>\n      <th>normalized_root_mean_squared_log_error</th>\n      <td>NaN</td>\n      <td>0.133442</td>\n      <td>NaN</td>\n      <td>0.091530</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.094345</td>\n      <td>NaN</td>\n      <td>0.140821</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>r2_score</th>\n      <td>0.596469</td>\n      <td>0.390106</td>\n      <td>0.337128</td>\n      <td>0.538474</td>\n      <td>0.556447</td>\n      <td>0.590382</td>\n      <td>0.568976</td>\n      <td>0.586969</td>\n      <td>0.313132</td>\n      <td>0.596707</td>\n    </tr>\n    <tr>\n      <th>root_mean_squared_error</th>\n      <td>57.028303</td>\n      <td>70.110482</td>\n      <td>73.092989</td>\n      <td>60.988988</td>\n      <td>59.789604</td>\n      <td>57.456003</td>\n      <td>58.939643</td>\n      <td>57.695387</td>\n      <td>74.404381</td>\n      <td>57.011321</td>\n    </tr>\n    <tr>\n      <th>root_mean_squared_log_error</th>\n      <td>NaN</td>\n      <td>0.840299</td>\n      <td>NaN</td>\n      <td>0.576376</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.594103</td>\n      <td>NaN</td>\n      <td>0.886766</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>spearman_correlation</th>\n      <td>0.815239</td>\n      <td>0.752286</td>\n      <td>0.618926</td>\n      <td>0.770375</td>\n      <td>0.786202</td>\n      <td>0.808953</td>\n      <td>0.797009</td>\n      <td>0.806546</td>\n      <td>0.650528</td>\n      <td>0.814794</td>\n    </tr>\n    <tr>\n      <th>spearman_correlation_max</th>\n      <td>0.815239</td>\n      <td>0.815239</td>\n      <td>0.815239</td>\n      <td>0.815239</td>\n      <td>0.815239</td>\n      <td>0.815239</td>\n      <td>0.815239</td>\n      <td>0.815239</td>\n      <td>0.815239</td>\n      <td>0.815239</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                                0          1          2  \\\nexplained_variance                       0.598114   0.390203   0.337219   \nmean_absolute_error                     41.158197  55.180982  57.217501   \nmedian_absolute_error                   29.480241  47.278671  48.135341   \nnormalized_mean_absolute_error           0.075938   0.101810   0.105567   \nnormalized_median_absolute_error         0.054392   0.087230   0.088811   \nnormalized_root_mean_squared_error       0.105218   0.129355   0.134858   \nnormalized_root_mean_squared_log_error        NaN   0.133442        NaN   \nr2_score                                 0.596469   0.390106   0.337128   \nroot_mean_squared_error                 57.028303  70.110482  73.092989   \nroot_mean_squared_log_error                   NaN   0.840299        NaN   \nspearman_correlation                     0.815239   0.752286   0.618926   \nspearman_correlation_max                 0.815239   0.815239   0.815239   \n\n                                                3          4          5  \\\nexplained_variance                       0.538539   0.556496   0.590431   \nmean_absolute_error                     45.047540  44.289862  42.028387   \nmedian_absolute_error                   33.973801  33.622797  31.027954   \nnormalized_mean_absolute_error           0.083114   0.081716   0.077543   \nnormalized_median_absolute_error         0.062682   0.062035   0.057247   \nnormalized_root_mean_squared_error       0.112526   0.110313   0.106007   \nnormalized_root_mean_squared_log_error   0.091530        NaN        NaN   \nr2_score                                 0.538474   0.556447   0.590382   \nroot_mean_squared_error                 60.988988  59.789604  57.456003   \nroot_mean_squared_log_error              0.576376        NaN        NaN   \nspearman_correlation                     0.770375   0.786202   0.808953   \nspearman_correlation_max                 0.815239   0.815239   0.815239   \n\n                                                6          7          8  \\\nexplained_variance                       0.569029   0.587016   0.313204   \nmean_absolute_error                     43.595751  42.269064  58.244588   \nmedian_absolute_error                   32.487259  31.280734  49.154011   \nnormalized_mean_absolute_error           0.080435   0.077987   0.107462   \nnormalized_median_absolute_error         0.059940   0.057714   0.090690   \nnormalized_root_mean_squared_error       0.108745   0.106449   0.137277   \nnormalized_root_mean_squared_log_error   0.094345        NaN   0.140821   \nr2_score                                 0.568976   0.586969   0.313132   \nroot_mean_squared_error                 58.939643  57.695387  74.404381   \nroot_mean_squared_log_error              0.594103        NaN   0.886766   \nspearman_correlation                     0.797009   0.806546   0.650528   \nspearman_correlation_max                 0.815239   0.815239   0.815239   \n\n                                                9  \nexplained_variance                       0.597787  \nmean_absolute_error                     41.221683  \nmedian_absolute_error                   29.660043  \nnormalized_mean_absolute_error           0.076055  \nnormalized_median_absolute_error         0.054723  \nnormalized_root_mean_squared_error       0.105187  \nnormalized_root_mean_squared_log_error        NaN  \nr2_score                                 0.596707  \nroot_mean_squared_error                 57.011321  \nroot_mean_squared_log_error                   NaN  \nspearman_correlation                     0.814794  \nspearman_correlation_max                 0.815239  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Register the best model \n\nUse the `regression_run` object to get the best model and register it into the workspace. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = regression_run.get_output()\n\n# register model in workspace\ndescription = 'Aml Model ' + regression_run.id[7:15]\ntags = None\nregression_run.register_model(description=description, tags=tags)\nregression_run.model_id # Use this id to deploy the model as a web service in Azure",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Registering model AutoMLe1d6a0766best\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "'AutoMLe1d6a0766best'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Save model information for deployment\n\nPersist the information that we will need to deploy the model in the [turbofan deploy model](./turbofan_deploy_model.ipynb)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nimport os\n\nmodel_information = {'regressionRunId': regression_run.id, 'modelId': regression_run.model_id, 'experimentName': experiment.name}\nwith open('./aml_config/model_config.json', 'w') as fo:\n  json.dump(model_information, fo)",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Load test data\n\nIn the test set, the time series ends some time prior to system failure. The actual\nremaining useful life (RUL) are given in the RUL_*.txt files.  The data in the RUL files is a single vector where the index corresponds to the unit number of the engine and the value corresponds to the actual RUL at the end of the test.\n\nThe RUL for a given cycle in the training set is given by adding the RUL at test end (from the RUL vector file) to the maximum cycle in the test data and then subtracting the current cycle:\n\n$$RUL_{current} =  RUL_{TestEnd} + Cycle_{max} - Cycle_{current}$$\n\nTaking unit number 1 as an example:\n   * Taking the first value from RUL_FD003.txt gives:       $RUL_{TestEnd} = 44$\n   * The final(max) cycle value from test_FD003.txt gives:  $Cycle_{max} = 233$\n   * The values for the first 5 cycles for engine 1 are:\n\n|Unit|Cycle|Max Cycle|Test End RUL|Remaining Life|\n|-----|-----|-----|-----|-----|\n|1|1|233|44|276|\n|1|2|233|44|275|\n|1|3|233|44|274|\n|1|4|233|44|273|\n|1|5|233|44|272|\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n## Define some methods for loading from text files"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nimport utils\nimport pandas as pd\nfrom os.path import isfile\n\ndef add_column_names(data_frame):\n    data_frame.columns = ([\"Unit\",\"CycleTime\"]\n                          + [\"OperationalSetting\"+str(i) for i in range(1,4)]\n                          + [\"Sensor\"+str(i) for i in range(1,22)])\n\ndef read_data_file(full_file_name):\n    data = pd.read_csv(full_file_name, sep = ' ', header = None)\n    data.dropna(axis='columns', inplace=True)\n    return data\n\ndef load_rul_data(full_file_name):\n    rul_data = read_data_file(full_file_name)\n\n    # add a column for the unit and fill with numbers 1..n where\n    # n = number of rows of RUL data\n    rul_data['Unit'] = list(range(1, len(rul_data) + 1))\n    rul_data.rename(columns = {0 : 'TestEndRUL'}, inplace = True)\n    return rul_data\n\n\ndef load_test_data(test_full_file_name, rul_full_file_name):\n    data = read_data_file(test_full_file_name)\n    add_column_names(data)\n    data = utils.add_maxcycle(data)\n    \n    rul_data = load_rul_data(rul_full_file_name)\n    data = data.merge(rul_data, how = 'left', left_on = 'Unit', right_on = 'Unit')\n    data['RUL'] = data.apply(lambda r: int(r['MaxCycle'] + r['TestEndRUL'] - r['CycleTime']), axis = 1)\n    data['RulClass'] = data.apply(lambda r: utils.classify_rul(r['RUL']), axis = 1)\n\n    return data\n",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Read and process the test data\n\n><font color=gray>_Note: the dataset should correspond to the dataset from deviceharness_</font>\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset = \"FD004\" \nrul_file_name = 'data/RUL_' + dataset + '.txt'\ntest_file_name = 'data/test_' + dataset + '.txt'\n\ntest_pd = load_test_data(test_file_name, rul_file_name)\ntest_pd.head(5)",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unit</th>\n      <th>CycleTime</th>\n      <th>OperationalSetting1</th>\n      <th>OperationalSetting2</th>\n      <th>OperationalSetting3</th>\n      <th>Sensor1</th>\n      <th>Sensor2</th>\n      <th>Sensor3</th>\n      <th>Sensor4</th>\n      <th>Sensor5</th>\n      <th>...</th>\n      <th>Sensor16</th>\n      <th>Sensor17</th>\n      <th>Sensor18</th>\n      <th>Sensor19</th>\n      <th>Sensor20</th>\n      <th>Sensor21</th>\n      <th>MaxCycle</th>\n      <th>TestEndRUL</th>\n      <th>RUL</th>\n      <th>RulClass</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>20.0072</td>\n      <td>0.7000</td>\n      <td>100.0</td>\n      <td>491.19</td>\n      <td>606.67</td>\n      <td>1481.04</td>\n      <td>1227.81</td>\n      <td>9.35</td>\n      <td>...</td>\n      <td>0.02</td>\n      <td>362</td>\n      <td>2324</td>\n      <td>100.00</td>\n      <td>24.31</td>\n      <td>14.7007</td>\n      <td>230</td>\n      <td>22</td>\n      <td>251</td>\n      <td>Full</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>24.9984</td>\n      <td>0.6200</td>\n      <td>60.0</td>\n      <td>462.54</td>\n      <td>536.22</td>\n      <td>1256.17</td>\n      <td>1031.48</td>\n      <td>7.05</td>\n      <td>...</td>\n      <td>0.02</td>\n      <td>306</td>\n      <td>1915</td>\n      <td>84.93</td>\n      <td>14.36</td>\n      <td>8.5748</td>\n      <td>230</td>\n      <td>22</td>\n      <td>250</td>\n      <td>Full</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>42.0000</td>\n      <td>0.8420</td>\n      <td>100.0</td>\n      <td>445.00</td>\n      <td>549.23</td>\n      <td>1340.13</td>\n      <td>1105.88</td>\n      <td>3.91</td>\n      <td>...</td>\n      <td>0.02</td>\n      <td>328</td>\n      <td>2212</td>\n      <td>100.00</td>\n      <td>10.39</td>\n      <td>6.4365</td>\n      <td>230</td>\n      <td>22</td>\n      <td>249</td>\n      <td>Full</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4</td>\n      <td>42.0035</td>\n      <td>0.8402</td>\n      <td>100.0</td>\n      <td>445.00</td>\n      <td>549.19</td>\n      <td>1339.70</td>\n      <td>1107.26</td>\n      <td>3.91</td>\n      <td>...</td>\n      <td>0.02</td>\n      <td>328</td>\n      <td>2212</td>\n      <td>100.00</td>\n      <td>10.56</td>\n      <td>6.2367</td>\n      <td>230</td>\n      <td>22</td>\n      <td>248</td>\n      <td>Full</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>5</td>\n      <td>35.0079</td>\n      <td>0.8400</td>\n      <td>100.0</td>\n      <td>449.44</td>\n      <td>555.10</td>\n      <td>1353.04</td>\n      <td>1117.80</td>\n      <td>5.48</td>\n      <td>...</td>\n      <td>0.02</td>\n      <td>333</td>\n      <td>2223</td>\n      <td>100.00</td>\n      <td>14.85</td>\n      <td>8.9326</td>\n      <td>230</td>\n      <td>22</td>\n      <td>247</td>\n      <td>Full</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 30 columns</p>\n</div>",
            "text/plain": "   Unit  CycleTime  OperationalSetting1  OperationalSetting2  \\\n0     1          1              20.0072               0.7000   \n1     1          2              24.9984               0.6200   \n2     1          3              42.0000               0.8420   \n3     1          4              42.0035               0.8402   \n4     1          5              35.0079               0.8400   \n\n   OperationalSetting3  Sensor1  Sensor2  Sensor3  Sensor4  Sensor5    ...     \\\n0                100.0   491.19   606.67  1481.04  1227.81     9.35    ...      \n1                 60.0   462.54   536.22  1256.17  1031.48     7.05    ...      \n2                100.0   445.00   549.23  1340.13  1105.88     3.91    ...      \n3                100.0   445.00   549.19  1339.70  1107.26     3.91    ...      \n4                100.0   449.44   555.10  1353.04  1117.80     5.48    ...      \n\n   Sensor16  Sensor17  Sensor18  Sensor19  Sensor20  Sensor21  MaxCycle  \\\n0      0.02       362      2324    100.00     24.31   14.7007       230   \n1      0.02       306      1915     84.93     14.36    8.5748       230   \n2      0.02       328      2212    100.00     10.39    6.4365       230   \n3      0.02       328      2212    100.00     10.56    6.2367       230   \n4      0.02       333      2223    100.00     14.85    8.9326       230   \n\n   TestEndRUL  RUL  RulClass  \n0          22  251      Full  \n1          22  250      Full  \n2          22  249      Full  \n3          22  248      Full  \n4          22  247      Full  \n\n[5 rows x 30 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Serialize test data\n\nSave off the data so that we can use it when we test the web service in the [turbofan deploy model](./turbofan_deploy_model.ipynb) notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_pd.to_csv('./data/WebServiceTest.csv')",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Test regression model\n\npredict on training and test set and calculate residual values"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "selected_features = [\"Sensor\"+str(i) for i in range(1,22)]\n\n#load the values used to train the model\nX_train = train_pd[selected_features].values\ny_train = train_pd['RUL'].values\n\n#predict and calculate residual values for train\ny_pred_train = fitted_model.predict(X_train)\ny_residual_train = y_train - y_pred_train\n\ntrain_pd['predicted'] = y_pred_train; \ntrain_pd['residual'] = y_residual_train\n\n#load the values from the test set\nX_test = test_pd[selected_features].values\ny_test = test_pd['RUL'].values\n\n#predict and calculate residual values for test\ny_pred_test = fitted_model.predict(X_test)\ny_residual_test = y_test - y_pred_test\n\ntest_pd['predicted'] = y_pred_test;\ntest_pd['residual'] = y_residual_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_pd[['Unit', 'RUL', 'predicted', 'residual']].head(5)\ntest_pd[['Unit', 'RUL', 'predicted', 'residual']].head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Predicted vs. actual\n\nPlot the predicted RUL against the actual RUL.  The dashed line represents the ideal model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfig, (ax1,ax2) = plt.subplots(nrows=2, sharex=True)\nfig.set_size_inches(16, 16)\n\nfont_size = 14\n\ng = sns.regplot(y='predicted', x='RUL', data=train_pd, fit_reg=False, ax=ax1)\nlim_set = g.set(ylim=(0, 500), xlim=(0, 500))\nplot = g.axes.plot([0, 500], [0, 500], c=\".3\", ls=\"--\");\n\nrmse = ax1.text(16,450,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_train, y_pred_train))), fontsize = font_size)\nr2 = ax1.text(16,425,'R2 Score = {0:.2f}'.format(r2_score(y_train, y_pred_train)), fontsize = font_size)\n\ng2 = sns.regplot(y='predicted', x='RUL', data=test_pd, fit_reg=False, ax=ax2)\nlim_set = g2.set(ylim=(0, 500), xlim=(0, 500))\nplot = g2.axes.plot([0, 500], [0, 500], c=\".3\", ls=\"--\");\n\nrmse = ax2.text(16,450,'RMSE = {0:.2f}'.format(np.sqrt(mean_squared_error(y_test, y_pred_test))), fontsize = font_size)\nr2 = ax2.text(16,425,'R2 Score = {0:.2f}'.format(r2_score(y_test, y_pred_test)), fontsize = font_size)\n\nptitle = ax1.set_title('Train data', size=font_size)\nxlabel = ax1.set_xlabel('Actual RUL', size=font_size)\nylabel = ax1.set_ylabel('Predicted RUL', size=font_size)\n\nptitle = ax2.set_title(\"Test data\", size=font_size)\nxlabel = ax2.set_xlabel('Actual RUL', size=font_size)\nylabel = ax2.set_ylabel('Predicted RUL', size=font_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Predicted vs. residual"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig, (ax1,ax2) = plt.subplots(nrows=2, sharex=True)\nfig.set_size_inches(16, 16)\n\nfont_size = 14\n\ng = sns.regplot(y='residual', x='predicted', data=train_pd, fit_reg=False, ax=ax1)\nlim_set = g.set(ylim=(-350, 350), xlim=(0, 350))\nplot = g.axes.plot([0, 350], [0, 0], c=\".3\", ls=\"--\");\n\ng2 = sns.regplot(y='residual', x='predicted', data=test_pd, fit_reg=False, ax=ax2)\nlim_set = g2.set(ylim=(-350, 350), xlim=(0, 350))\nplot = g2.axes.plot([0, 350], [0, 0], c=\".3\", ls=\"--\");\n\nptitle = ax1.set_title('Train data', size=font_size)\nxlabel = ax1.set_xlabel('Predicted RUL', size=font_size)\nylabel = ax1.set_ylabel('Residual', size=font_size)\n\nptitle = ax2.set_title(\"Test data\", size=font_size)\nxlabel = ax2.set_xlabel('Predicted RUL', size=font_size)\nylabel = ax2.set_ylabel('Residual', size=font_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Residual distribution\n\nPlot histogram and Q-Q plot for test and train data to check for normal distibution of residuals"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import statsmodels.api as sm\nimport scipy.stats as stats\n\nfig, (ax1,ax2) = plt.subplots(nrows=2, ncols= 2)\nfig.set_size_inches(16, 16)\n\ng = sns.distplot(train_pd['residual'], ax=ax1[0], kde=False)\ng = stats.probplot(train_pd['residual'], plot=ax1[1])\n\ng2 = sns.distplot(test_pd['residual'], ax=ax2[0], kde=False)\ng2 = stats.probplot(test_pd['residual'], plot=ax2[1])\n\n\nptitle = ax1[0].set_title('Residual Histogram Train', size=font_size)\nxlabel = ax1[0].set_xlabel('Residuals', size=font_size)\nptitle = ax1[1].set_title('Q-Q Plot Train Residuals', size=font_size)\n\nptitle = ax2[0].set_title('Residual Histogram Test', size=font_size)\nxlabel = ax2[0].set_xlabel('Residuals', size=font_size)\nptitle = ax2[1].set_title('Q-Q Plot Test Residuals', size=font_size)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Score model\n\nThe PHM08 scoring method is used to derive a score for the model. The method penalizes low predictions (i.e. negative residuals) more heavily than high predictions. The score only takes into account the models prediction of RUL for the last cycle of the test set."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def score(x):\n    diff = x.predicted-x.RUL\n    result = np.expm1(diff/-10.) if diff < 0. else np.expm1(diff/13.)\n    return result\n\nmax_unit = test_pd.groupby(['Unit']).agg({'CycleTime' : 'max'})\nmax_unit['Unit'] = list(range(1, len(max_unit) + 1))\n\nmax_unit = pd.merge(max_unit, test_pd, on=['CycleTime', 'Unit'])\n\nmax_unit[\"score\"] = max_unit.apply(score, axis=1)\nscore = sum(max_unit.score)\nround(score)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}